{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 6: классификация текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом домашнем задании вам предстоит построить классификатор текстов!\n",
    "\n",
    "Данные мы будем использовать из Kaggle соревнования: https://www.kaggle.com/competitions/nlp-getting-started/data Оттуда надо скачать файл train.csv. На обучающую и тестовую выборки его поделим кодом ниже, менять его не надо!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем работать с датасетом постов из твиттера. Нам предстоит решать задачу бинарной классификации - определять содержатся ли в твитте информация о настоящей катастрофе/инциденте или нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1 (0.5 балла)\n",
    "\n",
    "Выведете на экран информацию о пропусках в данных. Если пропуски присутствуют заполните их пустой строкой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5329, 5), (2284, 5))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       44\n",
       "location    1760\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            0\n",
       "keyword      17\n",
       "location    773\n",
       "text          0\n",
       "target        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/polinakrupica/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4317: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().fillna(\n"
     ]
    }
   ],
   "source": [
    "train.fillna('', inplace = True)\n",
    "test.fillna('', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "keyword     0\n",
       "location    0\n",
       "text        0\n",
       "target      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "keyword     0\n",
       "location    0\n",
       "text        0\n",
       "target      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2 (1 балл)\n",
    "Давайте немного посмотрим на наши данные. Визуализируйте (где явно просят) или выведете информацию о следующем:\n",
    "\n",
    "1. Какое распределение классов в обучающей выборке?\n",
    "2. Посмотрите на колонку \"keyword\" - возьмите 10 наиболее встречающихся значений, постройте ступенчатую диаграмму распределения классов в зависимости от значения keyword, сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3024\n",
       "1    2305\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43253893788703324"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(train['target'] == 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распределение классов достаточно равномерное, преобладает класс 0, но разница между классами небольшая. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>frequency</th>\n",
       "      <th>target0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>239253</td>\n",
       "      <td>32</td>\n",
       "      <td>44</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>damage</td>\n",
       "      <td>103512</td>\n",
       "      <td>17</td>\n",
       "      <td>36</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>siren</td>\n",
       "      <td>306832</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>wreckage</td>\n",
       "      <td>365815</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>fatalities</td>\n",
       "      <td>171214</td>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>deluge</td>\n",
       "      <td>98511</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>nuclear%20reactor</td>\n",
       "      <td>219620</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>derail</td>\n",
       "      <td>102750</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>emergency</td>\n",
       "      <td>135736</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>fatality</td>\n",
       "      <td>157083</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               keyword      id  target  frequency  target0\n",
       "0                       239253      32         44       12\n",
       "58              damage  103512      17         36       19\n",
       "179              siren  306832       4         35       31\n",
       "220           wreckage  365815      34         34        0\n",
       "105         fatalities  171214      16         33       17\n",
       "64              deluge   98511       5         31       26\n",
       "149  nuclear%20reactor  219620      12         30       18\n",
       "69              derail  102750      15         30       15\n",
       "91           emergency  135736      11         30       19\n",
       "106           fatality  157083       8         30       22"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = train.groupby('keyword').sum()\n",
    "df2 = train.keyword.value_counts().to_frame()\n",
    "result = pd.concat([df1, df2], axis=1).reset_index()\n",
    "result.rename(columns = {'keyword': 'frequency', 'index': 'keyword'}, inplace = True)\n",
    "result.sort_values(by = 'frequency', ascending = False, inplace = True)\n",
    "\n",
    "result = result.iloc[0:10]\n",
    "result['target0'] = result['frequency'] - result['target']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fed447326d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 3))\n",
    "plt.bar(x=result['keyword'], height=result['target'], alpha=0.5, label = 'класс 1')\n",
    "plt.bar(x = result['keyword'], height=result['target0'], alpha=0.5, label = 'класс 0')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "График показывает распределение классов 1 и 0 внутри каждого из 10 самых популярных ключевых слов. Очевидно, что класс 1 преобладает у слов: wreckage и у Nan, однако Nan в дальнейшем никакой информации нести не будет, его можно удалить при желании. В остальных случаях преобладает класс 0, но у таких слов, как derail, fatalities и damage классы распределены почти поровну.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3 (0.5 балла) \n",
    "\n",
    "В этом задании предлагается объединить все три текстовых столбца в один (просто сконкатенировать cтроки) и убрать столбец с индексом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-79350a5c1232>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['new'] = train['keyword'] + ' ' + train['location'] + ' ' + train['text']\n"
     ]
    }
   ],
   "source": [
    "train['new'] = train['keyword'] + ' ' + train['location'] + ' ' + train['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-db494f3c9e90>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['new'] = test['keyword'] + ' ' + test['location'] + ' ' + test['text']\n"
     ]
    }
   ],
   "source": [
    "test['new'] = test['keyword'] + ' ' + test['location'] + ' ' + test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/polinakrupica/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "train.drop(columns = ['id', 'keyword', 'location', 'text'], inplace = True)\n",
    "test.drop(columns = ['id', 'keyword', 'location', 'text'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4 (0.5 балла)\n",
    "\n",
    "Далее мы будем пока работать только с train частью.\n",
    "\n",
    "1. Предобработайте данные (train часть) с помощью CountVectorizer.\n",
    "2. Какого размера получилась матрица?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vec = CountVectorizer()\n",
    "X = cnt_vec.fit_transform(train['new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5329x18455 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 86671 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матрица получилась огромная - 5329x18455"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5 (1 балл)\n",
    "\n",
    "В предыдущем пункте у вас должна была получиться достаточно большая матрица.\n",
    "Если вы взгляните на текст, то увидете, что там есть множество специальных символов, ссылок и прочего мусора.\n",
    "\n",
    "Давайте также посмотрим на словарь, который получился в результате построения CountVectorizer, его можно найти в поле vocabulary_ инстанса этого класса. Давайте напишем функцию, которая печает ответы на следующие вопросы:\n",
    "\n",
    "1. Найдите в этом словаре все слова, которые содержат цифры. Сколько таких слов нашлось?\n",
    "\n",
    "2. Найдите все слова, которые содержат символы пунктуации. Сколько таких слов нашлось? \n",
    "\n",
    "3. Сколько хэштегов (токен начинается на #) и упоминаний (токен начинается на @) осталось в словаре?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bridge', 2948),\n",
       " ('20collapse', 320),\n",
       " ('ashes', 1928),\n",
       " ('2015', 295),\n",
       " ('australia', 2059),\n",
       " ('ûªs', 18425),\n",
       " ('collapse', 3914),\n",
       " ('at', 1977),\n",
       " ('trent', 16483),\n",
       " ('among', 1628),\n",
       " ('worst', 17813),\n",
       " ('in', 8314),\n",
       " ('history', 7773),\n",
       " ('england', 5722),\n",
       " ('bundled', 3085),\n",
       " ('out', 11995),\n",
       " ('for', 6503),\n",
       " ('60', 755),\n",
       " ('http', 7970),\n",
       " ('co', 3861),\n",
       " ('t5trhjuau0', 15733),\n",
       " ('hail', 7420),\n",
       " ('carol', 3363),\n",
       " ('stream', 15419),\n",
       " ('illinois', 8249),\n",
       " ('great', 7206),\n",
       " ('michigan', 10594),\n",
       " ('technique', 15888),\n",
       " ('camp', 3261),\n",
       " ('b1g', 2147),\n",
       " ('thanks', 16013),\n",
       " ('to', 16272),\n",
       " ('bmurph1019', 2753),\n",
       " ('hail_youtsey', 7421),\n",
       " ('termn8r13', 15949),\n",
       " ('goblue', 7088),\n",
       " ('wrestleon', 17848),\n",
       " ('oaskgki6qj', 11648),\n",
       " ('police', 12634),\n",
       " ('houston', 7933),\n",
       " ('cnn', 3854),\n",
       " ('tennessee', 15932),\n",
       " ('movie', 10940),\n",
       " ('theater', 16025),\n",
       " ('shooting', 14647),\n",
       " ('suspect', 15632),\n",
       " ('killed', 9246),\n",
       " ('by', 3150),\n",
       " ('di8elzswnr', 4890),\n",
       " ('rioting', 13800),\n",
       " ('still', 15357),\n",
       " ('couple', 4204),\n",
       " ('of', 11708),\n",
       " ('hours', 7925),\n",
       " ('left', 9644),\n",
       " ('until', 16884),\n",
       " ('have', 7546),\n",
       " ('be', 2383),\n",
       " ('up', 16895),\n",
       " ('class', 3774),\n",
       " ('wounds', 17822),\n",
       " ('lake', 9485),\n",
       " ('highlands', 7724),\n",
       " ('crack', 4242),\n",
       " ('the', 16022),\n",
       " ('path', 12228),\n",
       " ('where', 17582),\n",
       " ('wiped', 17684),\n",
       " ('this', 16107),\n",
       " ('morning', 10886),\n",
       " ('during', 5348),\n",
       " ('beach', 2386),\n",
       " ('run', 14049),\n",
       " ('surface', 15606),\n",
       " ('on', 11828),\n",
       " ('elbow', 5571),\n",
       " ('and', 1651),\n",
       " ('right', 13783),\n",
       " ('knee', 9316),\n",
       " ('yaqrsximph', 18081),\n",
       " ('airplane', 1458),\n",
       " ('20accident', 309),\n",
       " ('somewhere', 15025),\n",
       " ('there', 16070),\n",
       " ('experts', 5988),\n",
       " ('france', 6594),\n",
       " ('begin', 2442),\n",
       " ('examining', 5934),\n",
       " ('debris', 4641),\n",
       " ('found', 6553),\n",
       " ('reunion', 13707),\n",
       " ('island', 8601),\n",
       " ('french', 6642),\n",
       " ('air', 1451),\n",
       " ('accident', 1241),\n",
       " ('tagzbcxfj0', 15758),\n",
       " ('mlb', 10762),\n",
       " ('bloody', 2719),\n",
       " ('isolated', 8610),\n",
       " ('city', 3742)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cnt_vec.vocabulary_.items())[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(dict): \n",
    "    number_count = 0\n",
    "    punct_count = 0\n",
    "    hash_count = 0\n",
    "    for i in dict: \n",
    "        if re.search('\\d', i): \n",
    "            number_count += 1\n",
    "        if re.search('[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', i): \n",
    "            punct_count += 1\n",
    "        if  i[0] == '#': \n",
    "            hash_count += 1\n",
    "        if i[0] == '@': \n",
    "            hash_count += 1\n",
    "        #print(i, ind, (ind == 0 and i[0] == '#'))\n",
    "    return(number_count, punct_count, hash_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3812, 315, 0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaner(cnt_vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очевидно, что последний счетчик выдаст 0, тк CountVectorizer удалил все # и @ из начала слов. Также CountVectorizer удалил много знаков препинания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5329x18455 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 86671 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6 (0.5 балла)\n",
    "\n",
    "Вспомним, что на семинаре по текстам мы узнали, что в nltk есть специальный токенизатор для текстов - TweetTokenizer. Попробуем применить CountVectorizer с этим токенизатором. Ответьте на все вопросы из предыдущего пункта для TweetTokenizer и сравните результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "# Чтобы узнать, какие параметры есть у этого токенайзера - используйте help(TweetTokenizer)\n",
    "# Для того, чтобы передать токенайзер в CountVectorizer используйте параметр tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TweetTokenizer in module nltk.tokenize.casual:\n",
      "\n",
      "class TweetTokenizer(builtins.object)\n",
      " |  TweetTokenizer(preserve_case=True, reduce_len=False, strip_handles=False)\n",
      " |  \n",
      " |  Tokenizer for tweets.\n",
      " |  \n",
      " |      >>> from nltk.tokenize import TweetTokenizer\n",
      " |      >>> tknzr = TweetTokenizer()\n",
      " |      >>> s0 = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
      " |      >>> tknzr.tokenize(s0)\n",
      " |      ['This', 'is', 'a', 'cooool', '#dummysmiley', ':', ':-)', ':-P', '<3', 'and', 'some', 'arrows', '<', '>', '->', '<--']\n",
      " |  \n",
      " |  Examples using `strip_handles` and `reduce_len parameters`:\n",
      " |  \n",
      " |      >>> tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
      " |      >>> s1 = '@remy: This is waaaaayyyy too much for you!!!!!!'\n",
      " |      >>> tknzr.tokenize(s1)\n",
      " |      [':', 'This', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, preserve_case=True, reduce_len=False, strip_handles=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  tokenize(self, text)\n",
      " |      :param text: str\n",
      " |      :rtype: list(str)\n",
      " |      :return: a tokenized list of strings; concatenating this list returns        the original string if `preserve_case=False`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(TweetTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_vec = CountVectorizer(tokenizer = TweetTokenizer().tokenize)\n",
    "T = tw_vec.fit_transform(train['new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5329x19670 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 94563 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3939, 7337, 3149)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaner(tw_vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом,TweetTokenizer оставил гораздо больше знаков пунктуации, тк он не удалял смайлики и оставил упоминания и хэштеги в начале токенов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 7 (2 балла)\n",
    "\n",
    "В scikit-learn мы можем оценивать процесс подсчета матрицы через CountVectorizer. У CountVectorizer, как и у других наследников \\_VectorizerMixin, есть аргумент tokenizer и preprocessor. preprocessor применится в самом начале к каждой строке вашего датасета, tokenizer же должен принять строку и вернуть токены.\n",
    "Давайте напишем кастомный токенайзер, которые сделает все, что нам нужно: \n",
    "\n",
    "0. Приведет все буквы к нижнему регистру\n",
    "1. Разобьет текст на токены с помощью TweetTokenizer из пакета nltk\n",
    "2. Удалит все токены содержащие не латинские буквы, кроме смайликов (будем считать ими токены содержащие только пунктуацию и, как минимум, одну скобочку) и хэштегов, которые после начальной # содержат только латинские буквы.\n",
    "3. Удалит все токены, которые перечислены в nltk.corpus.stopwords.words('english')\n",
    "4. Проведет стемминг с помощью SnowballStemmer\n",
    "\n",
    "Продемонстрируйте работу вашей функции на первых десяти текстах в обучающей выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/polinakrupica/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def custom_token(line): \n",
    "    tw = TweetTokenizer()\n",
    "    from string import punctuation\n",
    "    a = line\n",
    "    a = a.lower()\n",
    "    a = tw.tokenize(a)\n",
    "    noise = list(punctuation) + nltk.corpus.stopwords.words('english')\n",
    "    for i in a: \n",
    "        if re.search(\"[^a-zA-Z]+\",i) and  not re.search(\"#[A-Za-z]+\", i) and not re.search('[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+[)(]', i ):\n",
    "            noise.append(i)\n",
    "    a = list(set(a) - set(noise))    \n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_a = [stemmer.stem(w) for w in a]\n",
    "    return(stemmed_a)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uluru', 'want', 'dust', '#spring', ':-)', ':)', ';(', 'alic']\n"
     ]
    }
   ],
   "source": [
    "# Это пример просто взятый из головы, чтобы показать, что смайлики он сохраняет, потому что в первых 10 строчках трейна нет смайликов\n",
    "\n",
    "tw = TweetTokenizer()\n",
    "from string import punctuation\n",
    "a = 'Dust @Storm e7n routeтт from Alice #Springs to Uluru http://t.co/4ilt6FXU45 #привки :)) .) :-) ;( wanted ûªs'\n",
    "a = a.lower()\n",
    "a = tw.tokenize(a)\n",
    "noise = list(punctuation) + nltk.corpus.stopwords.words('english')\n",
    "for i in a: \n",
    "    if re.search(\"[^a-zA-Z]+\",i) and  not re.search(\"#[A-Za-z]+\", i) and not re.search('[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+[)(]', i ):\n",
    "        noise.append(i)\n",
    "a = list(set(a) - set(noise))    \n",
    "stemmer = SnowballStemmer('english')\n",
    "stemmed_a = [stemmer.stem(w) for w in a]\n",
    "print(stemmed_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ash', 'trent', 'collaps', 'england', 'worst', 'among', 'histori', 'bridg', 'australia', 'bundl']\n",
      "['#goblu', '#wrestleon', 'michigan', 'thank', 'carol', 'hail', 'stream', 'techniqu', 'camp', 'illinoi', 'great']\n",
      "['houston', 'movi', 'kill', 'tennesse', 'suspect', 'shoot', 'polic', 'cnn', 'theater']\n",
      "['riot', 'left', 'still', 'class', 'coupl', 'hour']\n",
      "['left', 'crack', 'run', 'morn', 'highland', 'right', 'path', 'beach', 'lake', 'elbow', 'surfac', 'knee', 'wound', 'wipe']\n",
      "['#mlb', 'begin', 'reunion', 'expert', 'examin', 'french', 'air', 'island', 'accid', 'debri', 'franc', 'found', 'airplan', 'somewher']\n",
      "['citi', 'pakistani', 'video', 'perth', 'smirk', 'fun', 'killer', 'boast', 'came', 'world', 'indian', 'bloodi', 'isol', 'show', 'kill', 'remorseless']\n",
      "['except', 'realli', 'burn', 'idk']\n",
      "['destroy', 'ask', 'hous']\n",
      "['exchang', 'nirgua', 'venezuela', 'dead', 'suspect', 'polic', 'offic', 'wound', 'maracay', 'shot']\n"
     ]
    }
   ],
   "source": [
    "for i in train[:10]['new']: \n",
    "    print(custom_token(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bridge%20collapse  Ashes 2015: AustraliaÛªs collapse at Trent Bridge among worst in history: England bundled out Australia for 60 ... http://t.co/t5TrhjUAU0\n",
      "hail Carol Stream, Illinois GREAT MICHIGAN TECHNIQUE CAMP\n",
      "B1G THANKS TO @bmurph1019 \n",
      "@hail_Youtsey . @termn8r13 \n",
      "#GoBlue #WrestleOn http://t.co/OasKgki6Qj\n",
      "police Houston  CNN: Tennessee movie theater shooting suspect killed by police http://t.co/dI8ElZsWNR\n",
      "rioting  Still rioting in a couple of hours left until I have to be up for class.\n",
      "wounds Lake Highlands Crack in the path where I wiped out this morning during beach run. Surface wounds on left elbow and right knee. http://t.co/yaqRSximph\n",
      "airplane%20accident Somewhere Out There Experts in France begin examining airplane debris found on Reunion Island: French air accident experts on... http://t.co/TagZbcXFj0 #MLB\n",
      "bloody Isolated City In World Perth 'I came to kill Indians...for FUN': Video of smirking and remorseless Pakistani killer shows him boasting. http://t.co/FPjLwOXKlg\n",
      "burning  @JohnsonTionne except idk them?? it's really burning ??????\n",
      "destroy he/him or she/her (ask) destroy the house\n",
      "wounded Maracay y Nirgua, Venezuela Police Officer Wounded Suspect Dead After Exchanging Shots http://t.co/XxFk4KHbIw\n"
     ]
    }
   ],
   "source": [
    "for i in train[:10]['new']: \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 8 (1 балл)\n",
    "\n",
    "1. Примените CountVectorizer с реализованным выше токенизатором к обучающим и тестовым выборкам.\n",
    "2. Обучите LogisticRegression на полученных признаках.\n",
    "3. Посчитайте метрику f1-score на тестовых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = train['new']\n",
    "y_train = train['target']\n",
    "\n",
    "x_test = test['new']\n",
    "y_test = test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(tokenizer = custom_token)\n",
    "bow = vec.fit_transform(x_train) \n",
    "bow_test = vec.transform(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83      1318\n",
      "           1       0.79      0.71      0.75       966\n",
      "\n",
      "    accuracy                           0.80      2284\n",
      "   macro avg       0.79      0.79      0.79      2284\n",
      "weighted avg       0.80      0.80      0.79      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf.fit(bow, y_train)\n",
    "pred = clf.predict(bow_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 9 (1 балл)\n",
    "\n",
    "1. Повторите 7 задание, но с tf-idf векторизатором. Как изменилось качество?\n",
    "2. Мы можем еще сильнее уменьшить размер нашей матрицы, если отбросим значения df близкие к единице. Скорее всего такие слова не несут много информации о категории, так как встречаются достаточно часто. Ограничьте максимальный df в параметрах TfIdfVectorizer, поставьте верхнюю границу равную 0.9. Как изменился размер матрицы, как изменилось качество?\n",
    "3. Также мы можем уменьшить размер матрицы, удаляя слова со слишком маленьким df. Удалось ли добиться улучшения качества? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.90      0.84      1318\n",
      "           1       0.83      0.69      0.75       966\n",
      "\n",
      "    accuracy                           0.81      2284\n",
      "   macro avg       0.81      0.79      0.80      2284\n",
      "weighted avg       0.81      0.81      0.81      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec1 = TfidfVectorizer(tokenizer = custom_token)\n",
    "vec1_train = vec1.fit_transform(x_train)\n",
    "vec1_test = vec1.transform(x_test)\n",
    "clf = LogisticRegression(max_iter=300, random_state=42)\n",
    "clf.fit(vec1_train, y_train)\n",
    "pred_tfidf = clf.predict(vec1_test)\n",
    "print(classification_report(y_test, pred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество поднялось на одну сотую!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec2 = TfidfVectorizer(tokenizer = custom_token, max_df = 0.9)\n",
    "vec2_train = vec2.fit_transform(x_train)\n",
    "vec2_test = vec2.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5329x10571 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 50552 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.90      0.84      1318\n",
      "           1       0.83      0.69      0.75       966\n",
      "\n",
      "    accuracy                           0.81      2284\n",
      "   macro avg       0.81      0.79      0.80      2284\n",
      "weighted avg       0.81      0.81      0.81      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = LogisticRegression(max_iter=300, random_state=42)\n",
    "clf.fit(vec2_train, y_train)\n",
    "pred_tfidf = clf.predict(vec2_test)\n",
    "print(classification_report(y_test, pred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таблица никак не поменялась, потому что 0,9 это слишком высокий порог, нет таких слов, которые бы встречались в каждом предложении. В принципе в нашем датасете повторений слов должно быть видимо не очент много. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec3 = TfidfVectorizer(tokenizer = custom_token, max_df = 0.9, min_df = 0.005)\n",
    "vec3_train = vec3.fit_transform(x_train)\n",
    "vec3_test = vec3.transform(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.85      0.81      1318\n",
      "           1       0.77      0.67      0.71       966\n",
      "\n",
      "    accuracy                           0.77      2284\n",
      "   macro avg       0.77      0.76      0.76      2284\n",
      "weighted avg       0.77      0.77      0.77      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=300, random_state=42)\n",
    "clf.fit(vec3_train, y_train)\n",
    "pred_tfidf = clf.predict(vec3_test)\n",
    "print(classification_report(y_test, pred_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.90      0.84      1318\n",
      "           1       0.83      0.69      0.75       966\n",
      "\n",
      "    accuracy                           0.81      2284\n",
      "   macro avg       0.81      0.79      0.80      2284\n",
      "weighted avg       0.81      0.81      0.81      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec3 = TfidfVectorizer(tokenizer = custom_token, max_df = 0.9, min_df = 0.0001)\n",
    "vec3_train = vec3.fit_transform(x_train)\n",
    "vec3_test = vec3.transform(x_test)\n",
    "clf = LogisticRegression(max_iter=300, random_state=42)\n",
    "clf.fit(vec3_train, y_train)\n",
    "pred_tfidf = clf.predict(vec3_test)\n",
    "print(classification_report(y_test, pred_tfidf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Улучшиьть качество не удалось, при очень маленьких min_df ничего не меняется, а при больших - качество падает"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 10 (1 балл)\n",
    "\n",
    "Еще один популяпный трюк, который позволит уменьшить количество признаков называется hashing trick. Его суть в том, то мы случайно группируем признаки ииии  ..... складываем их! А потом удаляем исходные признаки. В итоге все наши признаки это просто суммы исходных. Звучит странно, но это отлично работает. Давайте проверим этот трюк в нашем сеттинге.\n",
    "Также при таком подходе вам не нужно хранить словарь token->index, что тоже иногда полезно.\n",
    "\n",
    "1. Повторите задание 7 с HashingVectorizer, укажите количество фичей равное 5000.\n",
    "2. Какой из подходов показал самый высокий результат?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/polinakrupica/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "hvec = HashingVectorizer(n_features=5000, tokenizer = custom_token) \n",
    "hvec_train = hvec.fit_transform(x_train)\n",
    "hvec_test = hvec.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.88      0.82      1318\n",
      "           1       0.80      0.66      0.72       966\n",
      "\n",
      "    accuracy                           0.78      2284\n",
      "   macro avg       0.79      0.77      0.77      2284\n",
      "weighted avg       0.79      0.78      0.78      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=300, random_state=42)\n",
    "clf.fit(hvec_train, y_train)\n",
    "pred_tfidf = clf.predict(hvec_test)\n",
    "print(classification_report(y_test, pred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый высокий результат показал в итоге подход с TfidfVectorizer, без верхних и нижних границ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 11 (1 балл)\n",
    "\n",
    "В этом задании нужно добиться f1 меры хотя в 0.75 на тестовых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У меня уже получилось это сделать. Например, используя TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.90      0.84      1318\n",
      "           1       0.83      0.69      0.75       966\n",
      "\n",
      "    accuracy                           0.81      2284\n",
      "   macro avg       0.81      0.79      0.80      2284\n",
      "weighted avg       0.81      0.81      0.81      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec1 = TfidfVectorizer(tokenizer = custom_token)\n",
    "vec1_train = vec1.fit_transform(x_train)\n",
    "vec1_test = vec1.transform(x_test)\n",
    "clf = LogisticRegression(max_iter=300, random_state=42)\n",
    "clf.fit(vec1_train, y_train)\n",
    "pred_tfidf = clf.predict(vec1_test)\n",
    "print(classification_report(y_test, pred_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
